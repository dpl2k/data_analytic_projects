{"cells":[{"cell_type":"markdown","id":"ef36f535-4bdc-4e2b-a22a-179372324b26","metadata":{},"source":["![walmartecomm](walmartecomm.jpg)\n","\n","Walmart is the biggest retail store in the United States. Just like others, they have been expanding their e-commerce part of the business. By the end of 2022, e-commerce represented a roaring $80 billion in sales, which is 13% of total sales of Walmart. One of the main factors that affects their sales is public holidays, like the Super Bowl, Labour Day, Thanksgiving, and Christmas. \n","\n","In this project, you have been tasked with creating a data pipeline for the analysis of supply and demand around the holidays, along with conducting a preliminary analysis of the data. You will be working with two data sources: grocery sales and complementary data. You have been provided with the `grocery_sales` table with the following features:\n","\n","# `grocery_sales`\n","- `\"index\"` - unique ID of the row\n","- `\"Store_ID\"` - the store number\n","- `\"Date\"` - the week of sales\n","- `\"Weekly_Sales\"` - sales for the given store\n","\n","Also, you have the `extra_data.parquet` file that contains complementary data:\n","\n","# `extra_data.parquet`\n","- `\"IsHoliday\"` - Whether the week contains a public holiday - 1 if yes, 0 if no.\n","- `\"Temperature\"` - Temperature on the day of sale\n","- `\"Fuel_Price\"` - Cost of fuel in the region\n","- `\"CPI\"` â€“ Prevailing consumer price index\n","- `\"Unemployment\"` - The prevailing unemployment rate\n","- `\"MarkDown1\"`, `\"MarkDown2\"`, `\"MarkDown3\"`, `\"MarkDown4\"` - number of promotional markdowns\n","- `\"Dept\"` - Department Number in each store\n","- `\"Size\"` - size of the store\n","- `\"Type\"` - type of the store (depends on `Size` column)\n","\n","You will need to merge those files and perform some data manipulations. The transformed DataFrame can then be stored as the `clean_data` variable containing the following columns:\n","- `\"Store_ID\"`\n","- `\"Month\"`\n","- `\"Dept\"`\n","- `\"IsHoliday\"`\n","- `\"Weekly_Sales\"`\n","- `\"CPI\"`\n","- \"`\"Unemployment\"`\"\n","\n","After merging and cleaning the data, you will have to analyze monthly sales of Walmart and store the results of your analysis as the `agg_data` variable that should look like:\n","\n","|  Month | Weekly_Sales  | \n","|---|---|\n","| 1.0  |  33174.178494 |\n","|  2.0 |  34333.326579 |\n","|  ... | ...  |  \n","\n","Finally, you should save the `clean_data` and `agg_data` as the csv files.\n","\n","It is recommended to use `pandas` for this project. "]},{"cell_type":"code","execution_count":1,"id":"c0d64ff1-a4ca-4a82-a8b4-e210244dedc1","metadata":{"collapsed":false,"executionCancelledAt":null,"executionTime":491,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1706805647975,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport numpy as np\nimport logging\nimport os\n\n# Start here...\nlogging.basicConfig(level=logging.INFO)\n\ndef extract(grocery_sales, parquet_file_path):\n    extra_data_df = pd.read_parquet(parquet_file_path)\n    merged_df = grocery_sales.merge(extra_data_df, on=['index'], how='inner')\n    return merged_df\n\ndef transform(merged_df):\n    merged_df.fillna({\\\n            'Weekly_Sales': merged_df['Weekly_Sales'].mean(),\\\n            'CPI': merged_df['CPI'].mean(),\\\n            'Unemployment': merged_df['Unemployment'].mean()\\\n        }, inplace = True)\n    merged_df['Month'] = merged_df['Date'].dt.month\n    merged_df = merged_df[merged_df['Weekly_Sales'] > 10000]\n    clean_data = merged_df[['Store_ID', 'Month', 'Dept', 'IsHoliday', 'Weekly_Sales', 'CPI', 'Unemployment']]\n    return clean_data\n\ndef avg_monthly_sales(clean_data):\n    agg_data = clean_data.groupby('Month')['Weekly_Sales'].mean().reset_index()\n    agg_data.columns = ['Month', 'Avg_Sales']\n    agg_data['Avg_Sales'] = agg_data['Avg_Sales'].round(2)\n    return agg_data\n\ndef load(clean_data, agg_data, clean_data_path, agg_data_path):\n    clean_data.to_csv(clean_data_path, index=False)\n    agg_data.to_csv(agg_data_path, index=False)\n\ndef validation(file_path):\n    if os.path.exists(file_path):\n        logging.info(f\"Validation successful: {file_path} exist.\")\n    else:\n        logging.error(f\"Validation failed: {file_path} do not exist.\")\n\nmerged_df = extract(grocery_sales, 'extra_data.parquet')\nclean_data = transform(merged_df)\nagg_data = avg_monthly_sales(clean_data)\nload(clean_data, agg_data, 'clean_data.csv', 'agg_data.csv')\nvalidation('clean_data.csv')\nvalidation('agg_data.csv')","outputsMetadata":{"0":{"height":584,"type":"stream"},"1":{"height":215,"type":"dataFrame"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 20000 entries, 0 to 19999\n","Data columns (total 5 columns):\n"," #   Column        Non-Null Count  Dtype         \n","---  ------        --------------  -----         \n"," 0   index         20000 non-null  int64         \n"," 1   Store_ID      20000 non-null  int64         \n"," 2   Date          19961 non-null  datetime64[ns]\n"," 3   Dept          20000 non-null  int64         \n"," 4   Weekly_Sales  19962 non-null  float64       \n","dtypes: datetime64[ns](1), float64(1), int64(3)\n","memory usage: 781.4 KB\n"]},{"name":"stderr","output_type":"stream","text":["INFO:root:Validation successful: clean_data.csv exist.\n","INFO:root:Validation successful: agg_data.csv exist.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import logging\n","import os\n","\n","# Start here...\n","logging.basicConfig(level=logging.INFO)\n","\n","grocery_sales = pd.read_csv('./grocery_sales.csv')\n","grocery_sales['Date'] = pd.to_datetime(grocery_sales['Date'])\n","grocery_sales.info()\n","\n","def extract(grocery_sales, parquet_file_path):\n","    extra_data_df = pd.read_parquet(parquet_file_path)\n","    merged_df = grocery_sales.merge(extra_data_df, on=['index'], how='inner')\n","    return merged_df\n","\n","def transform(merged_df):\n","    merged_df.fillna({\\\n","            'Weekly_Sales': merged_df['Weekly_Sales'].mean(),\\\n","            'CPI': merged_df['CPI'].mean(),\\\n","            'Unemployment': merged_df['Unemployment'].mean()\\\n","        }, inplace = True)\n","    merged_df['Month'] = merged_df['Date'].dt.month\n","    merged_df = merged_df[merged_df['Weekly_Sales'] > 10000]\n","    clean_data = merged_df[['Store_ID', 'Month', 'Dept', 'IsHoliday', 'Weekly_Sales', 'CPI', 'Unemployment']]\n","    return clean_data\n","\n","def avg_monthly_sales(clean_data):\n","    agg_data = clean_data.groupby('Month')['Weekly_Sales'].mean().reset_index()\n","    agg_data.columns = ['Month', 'Avg_Sales']\n","    agg_data['Avg_Sales'] = agg_data['Avg_Sales'].round(2)\n","    return agg_data\n","\n","def load(clean_data, agg_data, clean_data_path, agg_data_path):\n","    clean_data.to_csv(clean_data_path, index=False)\n","    agg_data.to_csv(agg_data_path, index=False)\n","\n","def validation(file_path):\n","    if os.path.exists(file_path):\n","        logging.info(f\"Validation successful: {file_path} exist.\")\n","    else:\n","        logging.error(f\"Validation failed: {file_path} do not exist.\")\n","\n","merged_df = extract(grocery_sales, 'extra_data.parquet')\n","clean_data = transform(merged_df)\n","agg_data = avg_monthly_sales(clean_data)\n","load(clean_data, agg_data, 'clean_data.csv', 'agg_data.csv')\n","validation('clean_data.csv')\n","validation('agg_data.csv')"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}
